{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crank-Nicolson With Rannacher Smoothing\n",
    "### Colin Alberts\n",
    "\n",
    "I've completed error analysis and finished defining all of the desired Greeks since last time. I also have begun implementing TR-BDF2 for Black-Scholes-Merton.\n",
    "\n",
    "Results that I still expect to show are completing an implementation of TR-BDF2 and comparing it with the Crank-Nicolson implementation with Rannacher smoothing. I will also complete error analysis on this. This should hypotetically be better since TR-BDF2 is L-stable instead of A-stable like Crank-Nicolson and preserves second order accuracy.\n",
    "\n",
    "I've been having some issues with the implementation thus far, but I hope to have it done by the middle of this week. The paper I am reading to implement it says TR-BDF2 is a weighted variation of Crank-Nicolson, but the details are murky."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Importing Packages and Defining Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0: Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import copy\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1: Defining Acronyms\n",
    "\n",
    "| Barrier Acronym | Barrier Name |\n",
    "|-----------------|-----------------|\n",
    "| UIC | Up-and-In Call |\n",
    "| UIP | Up-and-In Put |\n",
    "| UOC | Up-and-Out Call |\n",
    "| UOP | Up-and-Out Put |\n",
    "| DIC | Down-and-In Call |\n",
    "| DIP | Down-and-In Put |\n",
    "| DOC | Down-and-Out Call |\n",
    "| DOP | Down-and-Out Put |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1               # expiry time\n",
    "r = 0.1             # no-risk interest rate\n",
    "sigma = 0.3         # volatility of underlying asset\n",
    "delta = 0.0         # dividend rate\n",
    "E = 100.            # exercise price\n",
    "S_max = 2 * E       # upper bound of price of the stock (chosen somewhat arbitrarily, unlikely a security will double in price)\n",
    "S_min = 0\n",
    "B = 120             # barrier price\n",
    "N = int(T * 100)    # time iterations\n",
    "M = int(S_max * 4)  # space iterations, accurate to about 25 cents\n",
    "alpha = 2 - np.sqrt(2)  # TR-BDF2 time-stepping\n",
    "option_type = \"Euro Call\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time grid\n",
    "ht = T / N\n",
    "ht_small = ht / 4\n",
    "t_end = np.linspace(start = T - ht,\n",
    "                    stop = T,\n",
    "                    num = 5)\n",
    "t_start = np.linspace(start = 0,\n",
    "                      stop = T - ht,\n",
    "                      num = N - 1,\n",
    "                      endpoint = False)\n",
    "t = np.concatenate((t_start, t_end))\n",
    "\n",
    "# spatial grid (stock's price)\n",
    "s, hs = np.linspace(0,\n",
    "                    S_max,\n",
    "                    M + 1,\n",
    "                    retstep = True)\n",
    "\n",
    "\n",
    "def initial_condition(x, option):\n",
    "    global t\n",
    "    x_copy = x.copy()\n",
    "    \n",
    "    # control flow is basic/slower but sensible\n",
    "    if option == \"Euro Call\":\n",
    "        x_copy[:, -1] = np.maximum(s - E, 0)\n",
    "        x_copy[0, :] = 0\n",
    "        x_copy[-1, :] = np.exp(-r * (T - t)) * (S_max - E)\n",
    "    elif option == \"Euro Put\":\n",
    "        x_copy[:, -1] = np.maximum(E - s, 0)\n",
    "        x_copy[0, :] = np.exp(-r * (T - t)) * (E - S_min)\n",
    "        x_copy[-1, :] = 0\n",
    "    elif option in [\"DOC\", \"DIC\"]:\n",
    "        x_copy[:, -1] = np.maximum(s - E, 0) * np.where(s >= B, 1, 0)\n",
    "        x_copy[0, :] = 0\n",
    "        x_copy[-1, :] = np.exp(-r * (T - t)) * (S_max - E)\n",
    "    elif option in [\"UOC\", \"UIC\"]:\n",
    "        x_copy[:, -1] = np.maximum(s - E, 0) * np.where(s <= B, 1, 0)\n",
    "        x_copy[0, :] = x_copy[-1, :] = 0\n",
    "    elif option in [\"DOP\", \"DIP\"]:\n",
    "        x_copy[:, -1] = np.maximum(E - s, 0) * np.where(s >= B, 1, 0)\n",
    "        x_copy[0, :] = x_copy[-1, :] = 0\n",
    "    elif option in [\"UOP\", \"UIP\"]:\n",
    "        x_copy[:, -1] = np.maximum(E - s, 0) * np.where(s <= B, 1, 0)\n",
    "        x_copy[0, :] = np.exp(-r * (T - t)) * (E - S_min)\n",
    "        x_copy[-1, :] = 0\n",
    "\n",
    "    return x_copy\n",
    "\n",
    "# solution grid\n",
    "solution = np.zeros(shape = (M + 1, N + 1 + 3)) # the extra 3 is for Rannacher smoothing\n",
    "solution = initial_condition(solution, option_type)\n",
    "\n",
    "# averaging at strike to preserve second-order accuracy in Crank-Nicolson\n",
    "solution[int(np.ceil(M / 2))][-1] = (solution[int(np.ceil(M / 2)) - 1][-1] + solution[int(np.ceil(M / 2)) + 1][-1]) / 2\n",
    "\n",
    "# averaging at the barrier\n",
    "if option_type in [\"UIC\", \"UOC\", \"DIC\", \"DOC\",  \"UIP\", \"UOP\", \"DIP\", \"DOP\"]:\n",
    "    if B in s:\n",
    "        B_index = np.where(s == B)[0]\n",
    "        solution[B_index][-1] = (solution[B_index - 1][-1] + solution[B_index + 1][-1]) / 2\n",
    "    else:\n",
    "        # this is more concise than writing out a binary search\n",
    "        B_right_index = np.searchsorted(s, B)\n",
    "        B_left_index = B_right_index - 1\n",
    "        \n",
    "        # have to store temporary variables since I will be using each to average the other\n",
    "        temp = solution[B_right_index][-1]\n",
    "        solution[B_right_index][-1] = (solution[B_right_index - 1][-1] + solution[B_right_index + 1][-1]) / 2\n",
    "        solution[B_left_index][-1] = (solution[B_left_index - 1][-1] + temp) / 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2: Optimization Functions\n",
    "\n",
    "I used the Thomas Algorithm implementation from [this](https://github.com/katpirat/Finite-difference-for-PDEs/blob/main/FD_for_BlackScholes_pricing.py) repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thomas Algorithm, is only stable when matrix is diagonally dominant or SPD.\n",
    "def thomas(a, b, c, d):\n",
    "    dim = len(d)  # number of equations to be solved\n",
    "    ac, bc, cc, dc = map(np.array, (a, b, c, d))\n",
    "    \n",
    "    for i in range(1, dim):\n",
    "        w = ac[i - 1] / bc[i - 1]\n",
    "        bc[i] = bc[i] - w * cc[i - 1]\n",
    "        dc[i] = dc[i] - w * dc[i - 1]\n",
    "        \n",
    "    x = bc\n",
    "    x[-1] = dc[-1] / bc[-1]\n",
    "\n",
    "    for j in range(dim - 2, -1, -1):\n",
    "        x[j] = (dc[j] - cc[j] * x[j + 1]) / bc[j]\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I check for diagonal dominance in the matrix that I solve the system with. If it is not diagonally dominant then bad things happen (convergence issues, instability, increased computation cost, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking diagonal dominance for Thomas Algorithm stability\n",
    "def check_diag_dominant(mat):\n",
    "    diags = np.abs(np.diag(mat))\n",
    "    return np.all(diags >= np.sum(np.abs(mat), axis=1).T - diags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successive over-relaxation method is used for BDF2. I used the implementation from [this](https://github.com/enazari/iterative-methods-for-solving-linear-systems-in-python/blob/master/Successive_Overrelaxation_Method.ipynb) Github repo. This should be projected SOR. I used the Thomas Algorithm instead of PSOR for implementation. \n",
    "\n",
    "This code is technically \"dead\" but I will leave it for posterity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Successive over-relaxation implementation\n",
    "def sor(A, b, initial_guess):\n",
    "    # Decomposing the matrix A into a diagonal matrix, a strictly lower matrix, and a strictly upper matrix:\n",
    "    U = np.triu(A, 1)\n",
    "    L = np.tril(A, -1)\n",
    "    Diagonal = np.diag(A)\n",
    "    D = np.diagflat(Diagonal)\n",
    "    x = initial_guess\n",
    "    \n",
    "    # omega parameter must be between 0 and 2, otherwise the series diverges:\n",
    "    omega = 1.9 #2 - np.sqrt(2) # this is the value that the paper called for\n",
    "    \n",
    "    a1 = np.linalg.inv(D + omega * L)\n",
    "    a2 = omega * b\n",
    "    a3 = omega * U + (omega - 1) * D\n",
    "    \n",
    "    while True:\n",
    "        x = a1 @ (a2 - a3 @ x)\n",
    "        # Norm2:\n",
    "        if np.linalg.norm(A @ x-b, np.inf) < 5*10**(-4):\n",
    "            break\n",
    "    return x "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.0: Empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical(s, sigma, r, delta, option, time):\n",
    "    solution = np.zeros(shape = (s.shape[0], time.shape[0]))\n",
    "    \n",
    "    for i, current_s in enumerate(s):\n",
    "        d_1 = (np.log(current_s / E) + (r - delta + sigma ** 2 / 2) * (T - time)) / (sigma * np.sqrt(T - time))\n",
    "        d_2 = d_1 - sigma * np.sqrt(T - time)\n",
    "        \n",
    "        if option in [\"Euro Call\", \"UIC\", \"DIC\"]:\n",
    "            solution[i, :] = current_s * np.exp(-delta * (T - time)) * norm.cdf(d_1) - E * np.exp(-sigma * (T - time)) * norm.cdf(d_2)\n",
    "        elif option in [\"Euro Put\", \"UIP\", \"DIP\"]:\n",
    "            solution[i, :] = E * np.exp(-sigma * (T - time)) * norm.cdf(-d_2) - current_s * np.exp(-delta * (T - time)) * norm.cdf(-d_1)\n",
    "    \n",
    "    return solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1: Crank-Nicolson with Rannacher Smoothing\n",
    "\n",
    "The [link](https://www.researchgate.net/publication/228524629_Convergence_analysis_of_Crank-Nicolson_and_Rannacher_time-marching) that explains why I used quarter steps for ONE iteration in the Rannacher smoothing. See the last paragraph of page 107.\n",
    "\n",
    "Read [this](http://www.goddardconsulting.ca/option-pricing-finite-diff-crank-nicolson.html) to learn more about Crank-Nicolson differencing for Black-Scholes-Merton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crank_smoothed(s, sigma, r, delta, solution, option):\n",
    "    crank_solution = copy.deepcopy(solution)\n",
    "    \n",
    "    # Crank-Nicolson vectors\n",
    "    u_vec = (sigma ** 2 * s ** 2) / (4 * hs ** 2) - (r - delta) * s / (4 * hs)\n",
    "    v_vec = (sigma ** 2 * s ** 2) / (2 * hs ** 2) + r / 2\n",
    "    w_vec = (sigma ** 2 * s ** 2) / (4 * hs ** 2) + (r - delta) * s / (4 * hs)\n",
    "    \n",
    "    P_mat_C = -np.diag(u_vec[2:-1] * ht, -1) + np.diag(v_vec[1:-1] * ht) - np.diag(w_vec[1:-2] * ht, 1) + np.identity(M - 1)\n",
    "    \n",
    "    offset_1 = np.zeros(M - 1)\n",
    "    offset_2 = np.zeros(M - 1)\n",
    "    \n",
    "    # Backward Euler setup\n",
    "    alpha = ht_small * (-sigma ** 2 * s ** 2 / hs ** 2 + s * (r - delta)/ hs) / 2\n",
    "    beta = ht_small * (r + sigma ** 2 * s ** 2 / hs ** 2)\n",
    "    gamma = ht_small * (-s ** 2 * sigma ** 2 / hs ** 2 - s * (r - delta)/ hs) / 2\n",
    "    \n",
    "    P_mat_E = np.diag(alpha[2:-1], -1) + np.diag(beta[1:-1]) + np.diag(gamma[1:-2], 1) + np.identity(M - 1)\n",
    "    \n",
    "    offset = np.zeros(M - 1)\n",
    "    \n",
    "    for i in range(N + 3, 0, -1):\n",
    "        if i > N - 1:\n",
    "            if not check_diag_dominant(P_mat_E):\n",
    "                raise ValueError(\"Matrix is NOT diagonally dominant!!!\")\n",
    "            \n",
    "            offset[0], offset[-1] = crank_solution[0, i] * alpha[1], crank_solution[-1, i] * gamma[-1]\n",
    "            \n",
    "        \n",
    "            crank_solution[1:-1, i - 1] = thomas(np.diag(P_mat_E, -1),\n",
    "                                                 np.diag(P_mat_E),\n",
    "                                                 np.diag(P_mat_E, 1),\n",
    "                                                 crank_solution[1:-1, i] - offset)\n",
    "            \n",
    "        else:\n",
    "            if not check_diag_dominant(P_mat_C):\n",
    "                raise ValueError(\"Matrix is NOT diagonally dominant!!!\")\n",
    "                \n",
    "            offset_1[0] = crank_solution[0, i] * u_vec[1]\n",
    "            offset_1[-1] = crank_solution[-1, i] * w_vec[-1]\n",
    "\n",
    "            offset_2[0] = crank_solution[0, i - 1] * u_vec[1]\n",
    "            offset_2[-1] = crank_solution[-1, i - 1] * w_vec[-1]\n",
    "            \n",
    "            Q_mat = np.diag(u_vec[2:-1] * ht, -1) - np.diag(v_vec[1:-1] * ht) + np.diag(w_vec[1:-2] * ht, 1) + np.identity(M - 1)\n",
    "            RHS = np.dot(Q_mat, crank_solution[1:-1, i]) + ht * (offset_1 + offset_2)\n",
    "                \n",
    "            crank_solution[1:-1, i - 1] = thomas(np.diag(P_mat_C, -1),\n",
    "                                                np.diag(P_mat_C),\n",
    "                                                np.diag(P_mat_C, 1),\n",
    "                                                RHS)\n",
    "    \n",
    "    # keeping pass in for sanity check\n",
    "    if option_type in [\"UOC\", \"UOP\", \"DOC\", \"DOP\"]:\n",
    "        pass\n",
    "    elif option_type in [\"UIC\", \"DIP\", \"UIP\", \"DIC\"]:\n",
    "        crank_solution = empirical(s, sigma, r, delta, option, t) - crank_solution\n",
    "        \n",
    "    df = pd.DataFrame(crank_solution, columns = list(map(lambda i: str(ht * i), \n",
    "                                                         range(crank_solution.shape[1]))))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2: Crank-Nicolson (plain)\n",
    "Want Crank-Nicolson without smoothing for comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crank(s, sigma, r, delta, solution, option):\n",
    "    crank_solution = copy.deepcopy(solution)\n",
    "    \n",
    "    # Crank-Nicolson vectors\n",
    "    u_vec = (sigma ** 2 * s ** 2) / (4 * hs ** 2) - (r - delta) * s / (4 * hs)\n",
    "    v_vec = (sigma ** 2 * s ** 2) / (2 * hs ** 2) + r / 2\n",
    "    w_vec = (sigma ** 2 * s ** 2) / (4 * hs ** 2) + (r - delta) * s / (4 * hs)\n",
    "    \n",
    "    P_mat_C = -np.diag(u_vec[2:-1] * ht, -1) + np.diag(v_vec[1:-1] * ht) - np.diag(w_vec[1:-2] * ht, 1) + np.identity(M - 1)\n",
    "    \n",
    "    offset_1 = np.zeros(M - 1)\n",
    "    offset_2 = np.zeros(M - 1)\n",
    "    \n",
    "    for i in range(N, 0, -1):\n",
    "        if not check_diag_dominant(P_mat_C):\n",
    "            raise ValueError(\"Matrix is NOT diagonally dominant!!!\")\n",
    "                \n",
    "        offset_1[0] = crank_solution[0, i] * u_vec[1]\n",
    "        offset_1[-1] = crank_solution[-1, i] * w_vec[-1]\n",
    "\n",
    "        offset_2[0] = crank_solution[0, i - 1] * u_vec[1]\n",
    "        offset_2[-1] = crank_solution[-1, i - 1] * w_vec[-1]\n",
    "            \n",
    "        Q_mat = np.diag(u_vec[2:-1] * ht, -1) - np.diag(v_vec[1:-1] * ht) + np.diag(w_vec[1:-2] * ht, 1) + np.identity(M - 1)\n",
    "        RHS = np.dot(Q_mat, crank_solution[1:-1, i]) + ht * (offset_1 + offset_2)\n",
    "                \n",
    "        crank_solution[1:-1, i - 1] = thomas(np.diag(P_mat_C, -1),\n",
    "                                             np.diag(P_mat_C),\n",
    "                                             np.diag(P_mat_C, 1),\n",
    "                                             RHS)\n",
    "    \n",
    "    # keeping pass in for sanity check\n",
    "    if option_type in [\"UOC\", \"UOP\", \"DOC\", \"DOP\"]:\n",
    "        pass\n",
    "    elif option_type in [\"UIC\", \"DIC\", \"UIP\", \"DIP\"]:\n",
    "        crank_solution = empirical(s, sigma, r, delta, option, t) - crank_solution\n",
    "        \n",
    "    df = pd.DataFrame(crank_solution, columns = list(map(lambda i: str(ht * i), \n",
    "                                                         range(crank_solution.shape[1]))))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3: Trapezoidal Rule with Second-Order Backward Differentiation (TR-BDF2) \n",
    "Also known as the second-order accurate backward differencing scheme. From [this](https://chasethedevil.github.io/lefloch_trbdf2_draft.pdf) paper. Would be cool if these really smart people put their code in Github so I wouldn't have to fumble around with their poor explanations.\n",
    "\n",
    "Are the trapezoidal time-steps ghosts in the final solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trbdf2(s, sigma, r, delta, solution, option, alpha):\n",
    "    solution = copy.deepcopy(solution)\n",
    "    \n",
    "    # Trapezoidal (Crank-Nicolson) vectors\n",
    "    u_vec = (sigma ** 2 * s ** 2) / (4 * hs ** 2) - (r - delta) * s / (4 * hs)\n",
    "    v_vec = (sigma ** 2 * s ** 2) / (2 * hs ** 2) + r / 2\n",
    "    w_vec = (sigma ** 2 * s ** 2) / (4 * hs ** 2) + (r - delta) * s / (4 * hs)\n",
    "    \n",
    "    P_mat_C = -np.diag(u_vec[2:-1] * ht * alpha, -1) + np.diag(v_vec[1:-1] * ht * alpha) - np.diag(w_vec[1:-2] * ht * alpha, 1) + np.identity(M - 1)\n",
    "    \n",
    "    offset_1 = np.zeros(M - 1)\n",
    "    offset_2 = np.zeros(M - 1)\n",
    "    \n",
    "    # Crank-Nicolson for the alpha steps so the BDF2 has the values pre-processed\n",
    "    # There is probably a way to do CN and BDF2 with near-concurrency\n",
    "    for i in range(N - 1, -1, -1):\n",
    "        if not check_diag_dominant(P_mat_C):\n",
    "            raise ValueError(\"Matrix is NOT diagonally dominant!!!\")\n",
    "                    \n",
    "        offset_1[0] = solution[0, 2 * i + 2] * u_vec[1]\n",
    "        offset_1[-1] = solution[-1, 2 * i + 2] * w_vec[-1]\n",
    "\n",
    "        offset_2[0] = solution[0, 2 * i] * u_vec[1]\n",
    "        offset_2[-1] = solution[-1, 2 * i] * w_vec[-1]\n",
    "        \n",
    "        Q_mat = np.diag(u_vec[2:-1] * ht * alpha, -1) - np.diag(v_vec[1:-1] * ht * alpha) + np.diag(w_vec[1:-2] * ht * alpha, 1) + np.identity(M - 1)            \n",
    "        RHS = np.dot(Q_mat, solution[1:-1, 2 * i + 2]) + ht * (offset_1 + offset_2) * alpha\n",
    "                \n",
    "        solution[1:-1, 2 * i] = thomas(np.diag(P_mat_C, -1),\n",
    "                                           np.diag(P_mat_C),\n",
    "                                           np.diag(P_mat_C, 1),\n",
    "                                           RHS)\n",
    "    \n",
    "    offset_1 = np.zeros(M - 1)\n",
    "    offset_2 = np.zeros(M - 1)\n",
    "    \n",
    "    # BDF2\n",
    "    for i in range(N - 1, 0, -1):\n",
    "        offset_1[0] = solution[0, 2 * i + 1] * u_vec[1]\n",
    "        offset_1[-1] = solution[-1, 2 * i + 1] * w_vec[-1]\n",
    "\n",
    "        offset_2[0] = solution[0, 2 * i - 1] * u_vec[1]\n",
    "        offset_2[-1] = solution[-1, 2 * i - 1] * w_vec[-1]\n",
    "        \n",
    "        Q_mat = -np.diag(np.ones(M - 1)) * (1 - alpha) ** 2 / ((2 - alpha) * alpha)\n",
    "        RHS = np.dot(Q_mat, solution[1:-1, 2 * i + 1]) + 1 / ((2 - alpha) * alpha) * solution[1:-1, 2 * i] + ht * (offset_1 + offset_2) * alpha\n",
    "        \n",
    "        solution[1:-1, 2 * i - 1] = thomas(np.diag(P_mat_C, -1),\n",
    "                                       np.diag(P_mat_C),\n",
    "                                       np.diag(P_mat_C, 1),\n",
    "                                       RHS)\n",
    "        \n",
    "    # Keeping values in BDF2 since the Trapezoidals are ghosts/helper cells\n",
    "    solution = solution[:, ::2]\n",
    "    \n",
    "    # keeping pass in for sanity check\n",
    "    if option_type in [\"UOC\", \"UOP\", \"DOC\", \"DOP\"]:\n",
    "        pass\n",
    "    elif option_type in [\"UIC\", \"DIC\", \"UIP\", \"DIP\"]:\n",
    "        solution = empirical(s, sigma, r, delta, option, t) - solution\n",
    "    \n",
    "    df = pd.DataFrame(solution)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Plot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surface_plot(solution):\n",
    "    if solution.isnull().values.any():\n",
    "        raise ValueError(\"Null value in solution\")\n",
    "    \n",
    "    X, Y = np.meshgrid(t[::-1], s)\n",
    "    \n",
    "    fig = go.Figure(data=[go.Surface(z=solution, x=Y, y=X)])\n",
    "    \n",
    "    fig.update_layout(title='Price Surface', \n",
    "                      autosize=False,\n",
    "                      scene=dict(xaxis_title='Price',\n",
    "                                 yaxis_title='Time',\n",
    "                                 zaxis_title='Payoff'))\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Greeks\n",
    "\n",
    "Vega (*v*) and rho ($\\rho$) were omitted since we assume volatility and interest rate are constant. This is not realistic since these fluctuate; will omit for now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1: Delta\n",
    "\n",
    "$\\Delta = \\frac{\\partial V}{\\partial S}$. Sensitivity of option price with respect to price of the underlying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is solution grid, solution[price, time]\n",
    "def delta_greek(prices):\n",
    "    x, y = prices.shape\n",
    "    delta_matrix = np.zeros((x, y))\n",
    "    \n",
    "    # forward differencing for price = 0\n",
    "    delta_matrix[0, :] = (prices[1, :].T - prices[0, :].T) / hs\n",
    "    \n",
    "    # backward differencing for price = S_max\n",
    "    delta_matrix[-1, :] = (prices[-1, :].T - prices[-2, :].T) / hs\n",
    "    \n",
    "    # indexing to simplify centered differencing\n",
    "    K = np.arange(1, x - 1)\n",
    "    \n",
    "    # centered differencing for prices (0, S_max)\n",
    "    delta_matrix[K, :] = (prices[(K + 1), :] - prices[(K - 1), :]) / (2 * hs)\n",
    "    \n",
    "    return delta_matrix[:-2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(delta_greek(np.matrix(crank_sol)))\n",
    "# temp_sol = crank_sol.values\n",
    "# temp_delta = delta_greek(temp_sol)\n",
    "# surface_plot(temp_delta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2: Gamma\n",
    "\n",
    "$\\Gamma = \\frac{\\partial\\Delta}{\\partial S} = \\frac{\\partial ^ 2 V}{\\partial S ^ 2}$\n",
    "\n",
    "This is right but it looks weird because it makes sense if you zoom into the iterations. Looks similar to a normal curve with high variance.\n",
    "\n",
    "Sensitivity of $\\Delta$ with respect to the underlying price = concavity of option price with respect to price of the underlying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is solution grid, solution[price, time]\n",
    "def gamma_greek(prices):\n",
    "    x, y = prices.shape\n",
    "    gamma_matrix = np.zeros((x, y))\n",
    "    \n",
    "    # indexing to simplify centered differencing\n",
    "    K = np.arange(1, x - 1)\n",
    "    \n",
    "    # centered differencing for prices [0, S_max]\n",
    "    gamma_matrix[K, :] = (prices[(K + 1), :] - 2 * prices[K, :] + prices[(K - 1), :]) / (hs ** 2)\n",
    "    \n",
    "    return gamma_matrix[:-2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surface_plot(gamma_greek(crank_sol.values))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3: Theta\n",
    "$\\Theta = - \\frac{\\partial V}{\\partial\\tau}$. Sensitivity of option price with respect to time to expiry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is solution grid, solution[price, time]\n",
    "def theta_greek(prices):\n",
    "    x, y = prices.shape\n",
    "    theta_matrix = np.zeros((x, y))\n",
    "    \n",
    "    # indexing to simplify centered differencing\n",
    "    K = np.arange(1, y - 1)\n",
    "    \n",
    "    # centered differencing for times [0, T]\n",
    "    theta_matrix[:, K] = (prices[:, (K - 1)] - prices[:, (K + 1)]) / (2 * (t[K + 1] - t[K - 1]))\n",
    "    \n",
    "    return theta_matrix[:, 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surface_plot(theta_greek(crank_sol.values))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4: Lambda\n",
    "ABANDONED THIS GREEK!!!!!!!!!!!!!!!\n",
    "\n",
    "$\\lambda = \\Delta * \\frac{S}{V} = \\frac{\\partial V}{\\partial S} * \\frac{S}{V}$. Option leverage = change in the option value per $1\\%$ change in the underlying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input is solution grid, solution[price, time]\n",
    "# def lambda_greek(prices):\n",
    "#     x, y = prices.shape\n",
    "#     lambda_matrix = np.zeros((x, y))\n",
    "    \n",
    "#     # forward differencing for price = 0\n",
    "#     lambda_matrix[0, :] = (prices[1, :].T - prices[0, :].T) / hs\n",
    "    \n",
    "#     # backward differencing for price = S_max\n",
    "#     lambda_matrix[-1, :] = (prices[-1, :].T - prices[-2, :].T) / hs\n",
    "    \n",
    "#     # indexing to simplify centered differencing\n",
    "#     K = np.arange(1, x - 1)\n",
    "    \n",
    "#     # centered differencing for prices (0, S_max)\n",
    "#     lambda_matrix[K, :] = (prices[(K + 1), :] - prices[(K - 1), :]) / (2 * hs)\n",
    "    \n",
    "#     return lambda_matrix * s[K, :] / prices[K, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: CN with Rannacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crank_sol = crank_smoothed(s, sigma, r, delta, solution, option_type)\n",
    "\n",
    "# surface_plot(crank_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crank_cross_section_data = crank_sol.iloc[-2, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to reset t and grid for following methods\n",
    "t = np.linspace(start = 0,\n",
    "                stop = T,\n",
    "                num = N + 1)\n",
    "# solution grid\n",
    "solution = np.zeros(shape = (M + 1, N + 1))\n",
    "solution = initial_condition(solution, option_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_crank_sol = crank(s, sigma, r, delta, solution, option_type)\n",
    "# surface_plot(plain_crank_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_crank_cross_section_data = plain_crank_sol.iloc[-2, :]\n",
    "# plain_crank_cross_section, = plt.plot(t, plain_crank_cross_section_data, color = \"green\", label = \"Regular CN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3: Empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empirical_sol = empirical(s, sigma, r, delta, \"Euro Call\", t)\n",
    "# surface_plot(empirical_sol)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4: TR-BFD2\n",
    "Need to do setup again because the timestepping is $0, \\alpha, 1, 1 + \\alpha, 2, \\dots$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time grid: this method needs different time grid since it is split into solving for the alpha time steps and the regular time steps\n",
    "t_normal = np.linspace(start = 0,\n",
    "                        stop = T,\n",
    "                        num = N + 1)\n",
    "t_alpha = np.linspace(start = 0,\n",
    "                        stop = T,\n",
    "                        num = N,\n",
    "                        endpoint = False) + alpha / N\n",
    "t = np.array(sorted(np.concatenate((t_normal, t_alpha))))\n",
    "ht = t_normal[1] - t_normal[0]\n",
    "\n",
    "# solution grid\n",
    "solution = np.zeros(shape = (M + 1, int(2 * N + 1)))\n",
    "solution = initial_condition(solution, option_type)\n",
    "\n",
    "trbdf2_sol = trbdf2(s, sigma, r, delta, solution, option_type, alpha)\n",
    "surface_plot(trbdf2_sol)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Error Analysis\n",
    "All analysis will be for European Call option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(start = 0,\n",
    "                stop = T,\n",
    "                num = N + 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Generating Crank-Nicolson vs Crank-Nicolson with Rannacher Smoothing (& TR-BDF2) Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1: Initializing Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # risk-free and volatility grid\n",
    "# r_grid = np.linspace(0.01, 0.2, 20)\n",
    "# sigma_grid = np.linspace(0.01, 0.8, 80)\n",
    "\n",
    "# # initializing data frame\n",
    "# error_df = pd.DataFrame({\"Risk-Free Rate\": np.concatenate((r_grid, [0.1 for i in range(len(sigma_grid))])),\n",
    "#                          \"Implied Volatility\": np.concatenate(([0.3 for i in range(len(r_grid))], sigma_grid))})\n",
    "# error_df[\"Relative Error TRBDF2\"] = error_df[\"Relative Error CN\"] = error_df[\"Relative Error CNwRS\"] = float(\"inf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2: Relative Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Will make it easier to vectorize\n",
    "# def relative_error(df, method, t, solution):\n",
    "#     global s\n",
    "\n",
    "#     if method == \"CNwRS\":\n",
    "#         approximate_val = crank_smoothed(s, \n",
    "#                                          df[\"Implied Volatility\"], \n",
    "#                                          df[\"Risk-Free Rate\"], \n",
    "#                                          delta = 0, \n",
    "#                                          solution = solution, \n",
    "#                                          option = \"Euro Call\")\n",
    "#     elif method == \"CN\":\n",
    "#         approximate_val = crank(s, \n",
    "#                                 df[\"Implied Volatility\"], \n",
    "#                                 df[\"Risk-Free Rate\"], \n",
    "#                                 delta = 0, \n",
    "#                                 solution = solution, \n",
    "#                                 option = \"Euro Call\")\n",
    "#     elif method == \"TRBDF2\":\n",
    "#         approximate_val = trbdf2(s, \n",
    "#                                  df[\"Implied Volatility\"], \n",
    "#                                  df[\"Risk-Free Rate\"], \n",
    "#                                  delta = 0, \n",
    "#                                  solution = solution, \n",
    "#                                  option = \"Euro Call\",\n",
    "#                                  alpha = alpha)\n",
    "#         # have to change the grid for TR-BDF2 since the time grid of the input is different than the output\n",
    "#         t = np.linspace(start = 0, stop = T, num = N + 1)\n",
    "        \n",
    "#     empirical_val = empirical(s, \n",
    "#                               df[\"Implied Volatility\"], \n",
    "#                               df[\"Risk-Free Rate\"], \n",
    "#                               delta = 0, \n",
    "#                               option = \"Euro Call\", \n",
    "#                               time = t)\n",
    "    \n",
    "#     return np.sum(np.sum(np.abs(np.nan_to_num(empirical_val - approximate_val)))) / np.sum(np.sum(np.abs(np.nan_to_num(empirical_val))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3: Calculating Relative Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This could be more efficient using a more slick function and vectorization; this is fine for now.\n",
    "\n",
    "# # reset t and grid for CNwRS\n",
    "# t_end = np.linspace(start = T - ht, stop = T, num = 5);t_start = np.linspace(start = 0, stop = T - ht, num = N - 1, endpoint = False); t = np.concatenate((t_start, t_end))\n",
    "# solution = np.zeros(shape = (M + 1, N + 1 + 3)); solution = initial_condition(solution, option_type); solution[int(np.ceil(M / 2))][-1] = (solution[int(np.ceil(M / 2)) - 1][-1] + solution[int(np.ceil(M / 2)) + 1][-1]) / 2\n",
    "# for index, row in error_df.iterrows():\n",
    "#     error_df.at[index, 'Relative Error CNwRS'] = relative_error(row, \"CNwRS\", t, solution)\n",
    "\n",
    "# # reset t and grid for TR-BDF2\n",
    "# t_normal = np.linspace(start = 0, stop = T, num = N + 1); t_alpha = np.linspace(start = 0, stop = T, num = N, endpoint = False) + alpha / N\n",
    "# t = np.array(sorted(np.concatenate((t_normal, t_alpha)))); solution = np.zeros(shape = (M + 1, int(2 * N + 1)))\n",
    "# solution = initial_condition(solution, option_type)\n",
    "# for index, row in error_df.iterrows():\n",
    "#     error_df.at[index, 'Relative Error TRBDF2'] = relative_error(row, \"TRBDF2\", t, solution)\n",
    "    \n",
    "# # need to reset t and grid for plain Crank-Nicolson\n",
    "# t = np.linspace(start = 0, stop = T, num = N + 1); solution = np.zeros(shape = (M + 1, N + 1)); solution = initial_condition(solution, option_type)\n",
    "# for index, row in error_df.iterrows():\n",
    "#     error_df.at[index, 'Relative Error CN'] = relative_error(row, \"CN\", t, solution)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4: Calculating Error Difference Between CNwRS and CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # finding difference between CNwRS and CN\n",
    "# error_df[\"Error Diff CNwRS vs CN\"] = error_df[\"Relative Error CNwRS\"] - error_df[\"Relative Error CN\"]\n",
    "\n",
    "# # finding difference between TR-BDF2 and CN\n",
    "# error_df[\"Error Diff TR-BDF2 vs CN\"] = error_df[\"Relative Error TRBDF2\"] - error_df[\"Relative Error CN\"]\n",
    "\n",
    "# # finding difference between TR-BDF2 and CNwRS\n",
    "# error_df[\"Error Diff TR-BDF2 vs CNwRS\"] = error_df[\"Relative Error TRBDF2\"] - error_df[\"Relative Error CNwRS\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5: Saving DataFrame to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # saving df; compute is long\n",
    "# error_df.to_csv('relative_error_comparison.csv', index=False) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Error Analysis\n",
    "Negative \"Error Diff\" means that the method with Rannacher smoothing or TR-BDF2 is more accurate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.0: Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = pd.read_csv(\"relative_error_comparison.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1: Risk-Free Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering so implied volatility is constant\n",
    "risk_free_error_df = error_df.loc[error_df[\"Implied Volatility\"] == 0.3]\n",
    "risk_free_error_df = risk_free_error_df[:-1] # drop_duplicates was not working for some reason\n",
    "\n",
    "# risk-free error diff\n",
    "plt.plot(risk_free_error_df[\"Risk-Free Rate\"], risk_free_error_df[\"Error Diff CNwRS vs CN\"])\n",
    "plt.xlabel('Risk-Free Rate')\n",
    "plt.ylabel('Error Difference')\n",
    "plt.title('A Plot of Relative Error Difference vs Risk-Free Rate')\n",
    "plt.show()\n",
    "\n",
    "biggest_index = np.argmax(risk_free_error_df[\"Error Diff CNwRS vs CN\"])\n",
    "print(\"The smallest difference between Crank-Nicolson and Crank-Nicolson with Rannacher Smoothing occurs when r = \", np.round(np.array(risk_free_error_df[\"Risk-Free Rate\"])[biggest_index], 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that as the risk-free rate increases from $0$ to $1$ that the difference in error between Crank-Nicolson and Crank-Nicolson with Rannacher smoothing tends to $0$ in a linear manner. It seems as though it would become positive if the risk-free rate becomes great enough. This is unlikely to occur since risk-free rate is nearly always under $20\\%$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2: Implied Volatility Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering so implied volatility is constant\n",
    "vol_error_df = error_df.loc[error_df[\"Risk-Free Rate\"] == 0.1]\n",
    "\n",
    "# risk-free error diff\n",
    "plt.plot(vol_error_df[\"Implied Volatility\"], vol_error_df[\"Error Diff CNwRS vs CN\"])\n",
    "plt.xlabel('Implied Volatility')\n",
    "plt.ylabel('Error Difference')\n",
    "plt.title('A Plot of Relative Error Difference vs Implied Volatility')\n",
    "plt.ylim(min(vol_error_df[\"Error Diff CNwRS vs CN\"]) - 0.00025, 0)\n",
    "plt.show()\n",
    "\n",
    "biggest_index = np.argmax(vol_error_df[\"Error Diff CNwRS vs CN\"])\n",
    "print(\"The smallest difference between Crank-Nicolson and Crank-Nicolson with Rannacher Smoothing occurs when sigma = \", np.round(np.array(vol_error_df[\"Implied Volatility\"])[biggest_index], 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the difference in error between the methods converges to around $0.5\\%$ as implied volatility converges to $1$ and achieves a maximum where implied volatility is $10\\%$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3: Comparing Method Oscillations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, crank_cross_section_data[:-3], color = \"red\", label = \"CN with Rannacher steps\", linewidth = 2)\n",
    "plt.plot(t, plain_crank_cross_section_data, color = \"green\", label = \"Regular CN\")\n",
    "\n",
    "plt.legend(loc = \"upper left\")\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Payoff')\n",
    "plt.title('A Plot Comparing Smoothed vs Unsmoothed Crank-Nicolson')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows regular Crank-Nicolson's oscillations. The lack of (significant) oscillations with Rannacher smoothing increases accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4: Crank-Nicolson Error Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4.1: Risk-Free Rate Varies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_relative_error_r = risk_free_error_df[\"Relative Error CN\"]\n",
    "\n",
    "plt.plot(risk_free_error_df[\"Risk-Free Rate\"], cn_relative_error_r)\n",
    "\n",
    "plt.xlabel('Risk-Free Rate')\n",
    "plt.ylabel('Relative Error')\n",
    "plt.title('A Plot of Relative Error for Crank-Nicolson vs Risk-Free Rate')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "cn_relative_error_r.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative error decreases linearly as risk-free rate increases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4.2: Implied Volatility Varies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_relative_error_sig = vol_error_df[\"Relative Error CN\"]\n",
    "\n",
    "plt.plot(vol_error_df[\"Implied Volatility\"], cn_relative_error_sig)\n",
    "\n",
    "plt.xlabel('Implied Volatility')\n",
    "plt.ylabel('Relative Error')\n",
    "plt.title('A Plot of Relative Error for Crank-Nicolson vs Implied Volatility')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "cn_relative_error_sig.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative error tends to increase as implied volatility increases in a $\\log$-like manner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5: Crank-Nicolson With Rannacher Smoothing Error Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.5.1: Risk-Free Rate Varies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnrs_relative_error_r = risk_free_error_df[\"Relative Error CNwRS\"]\n",
    "\n",
    "plt.plot(risk_free_error_df[\"Risk-Free Rate\"], cnrs_relative_error_r)\n",
    "\n",
    "plt.xlabel('Risk-Free Rate')\n",
    "plt.ylabel('Relative Error')\n",
    "plt.title('A Plot of Relative Error for CNwRS vs Risk-Free Rate')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "cnrs_relative_error_r.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreases linearly, similar to plain Crank-Nicolson case. Error is slightly less."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.5.2: Implied Volatility Varies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnrs_relative_error_sig = vol_error_df[\"Relative Error CNwRS\"]\n",
    "\n",
    "plt.plot(vol_error_df[\"Implied Volatility\"], cnrs_relative_error_sig)\n",
    "\n",
    "plt.xlabel('Implied Volatility')\n",
    "plt.ylabel('Relative Error')\n",
    "plt.title('A Plot of Relative Error for CNwRS vs Implied Volatility')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "cnrs_relative_error_sig.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar shape as Crank-Nicolson relative error with slightly lower error values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
